---
title: "Machine Learning in public health"
format: revealjs
editor: visual
execute:
  eval: false
editor_options: 
  chunk_output_type: console
---

## Quarto

Agenda

- Science
- Three basic ideas
- Applications

## What is the model is wrong

```{r}
#| eval: true
library(BreakNBuild)
library(tidymodels)
library(tidyverse)
library(here)
```


## animate 

![](animation.gif)


## Simulated datasets

**Linear relation Dataset:** The first dataset simulates a scenario where the relationship between the predictors and the outcome is linear. The dataset consists of 400 observations with five predictors (`X1, X2, X3, X4, X5`), generated from a standard normal distribution. The outcome variable (`Y_linear`) is constructed as a linear combination of these predictors with coefficients (`β = [.1, -.2, .3, -.4, .5]`) and an added Gaussian noise. The linear model is represented as:

   `Y_linear = (.1 * X1) + (-.2 * X2) + (.3 * X3) + (-.4 * X4) + (.5 * X5) + ε`

where `ε` denotes the noise term following a normal distribution.

```{r}

set.seed(2023) # for reproducibility
n <- 500 # sample size
X <- matrix(rnorm(n * 5), ncol = 5) # 5 predictors
beta <- c(.3, -.2, .4, -.1, .2) # coefficients
y_linear <- X %*% beta + rnorm(n) # linear combination of X and beta with some noise
df_linear  <- cbind(y_linear, X)  |> as_tibble() 
colnames(df_linear)  <- c("Y", "X1", "X2", "X3", "X4", "X5")


```

The next Figure represents the correlation between the predictors and the outcome variable

```{r corr-linear, }
GGally::ggpairs(df_linear) 
ggsave(here("R", "img", "corr-linear.png"), 
       width = 10, height = 10, units = "in",
        dpi = 300)

```

![](`r here("R","img", "corr-linear.png")`)

**Interaction Structure Dataset:** Recognizing the importance of interaction effects in real-world data, the second dataset incorporates interaction terms between predictors. Alongside the original five predictors, two interaction terms are introduced: the product of `X1` and `X1`, and the product of `X3` and `X3`. The outcome variable (`Y_interaction`) is formulated by extending the linear model to include these interaction terms. This dataset challenges the model's ability to capture non-linear associations. The interaction coefficients are defined as follows:

   `Y_interaction = (.3 * X1) + (-.2 * X2) + (.4 * X3) + (-.1 * X4) + (.2 * X5) + (.6 * X1 * X1) + (.8 * X3 * X3) + ε`

Notice that the interaction terms are more strongly associated with the outcome than the original predictors, which implies that the model's performance will be highly dependent on its ability to capture these interactions.

```{r}

# Add interaction terms
X_interaction <- cbind(X[,1], X[,2], X[,3], X[,4], X[,5], X[,1] * X[,1], X[,3] * X[,3])

beta_interaction <- c(.3, -.2, .4, -.1, .2, .6, .8)

y_interaction <- X_interaction %*% beta_interaction + rnorm(n)

# Combine into a tibble
df_interaction <- cbind(y_interaction, X_interaction) |> as_tibble() |> select(-V7, -V8)
colnames(df_interaction) <- c("Y", "X1", "X2", "X3", "X4", "X5")

```

```{r corr-interaction}
GGally::ggpairs(df_interaction)
ggsave(here("R", "img", "corr-interaction.png"), 
       width = 10, height = 10, units = "in",
        dpi = 300)

```

Figure 2 represents the correlation between the predictors and the outcome variable

![](`r here("R","img", "corr-interaction.png")`)

**Random Dataset:** To establish a baseline for model performance, the third dataset is generated without any underlying systematic relationship between predictors and outcome. The outcome variable (`Y_random`) is random, following a normal distribution. This dataset serves as a control, enabling the evaluation of model performance in scenarios where the underlying data structure is non-informative.

`Y_random = ε`

```{r}

y_random <- rnorm(n)
# Combine into a tibble
df_random <- cbind(y_random, X) %>% as_tibble()
colnames(df_random) <- c("Y", "X1", "X2", "X3", "X4", "X5")

```

```{r corr-random}
GGally::ggpairs(df_random)
ggsave(here("R", "img", "corr-random.png"), 
       width = 10, height = 10, units = "in",
        dpi = 300)

```

Figure 3 represents the correlation between the predictors and the outcome variable

![](`r here("R","img", "corr-random.png")`)


## Code

```{r}
library(BreakNBuild)

linear_spec  <- 
    linear_reg()  |> 
    set_engine("lm")   |> 
    set_mode("regression")

svm_spec <-
    svm_poly(degree=2) |> 
    set_engine("kernlab")  |> 
    set_mode("regression")

tree_spec  <- 
    decision_tree()  |>  
    set_engine("rpart")  |> 
    set_mode("regression")

```

```{r}
random_splits  <- progressive_splits(df_random, assessment_size= 0.2, start_size = 3)

random_recipe <- recipe(Y ~ ., data = df_random)

### Linear dataset splits

linear_splits  <- progressive_splits(df_linear, assessment_size= 0.2, start_size = 3)

linear_recipe <- recipe(Y ~ ., data = df_linear)

### Interaction dataset splits

interaction_splits  <- progressive_splits(df_interaction, assessment_size= 0.2, start_size = 3)

interaction_recipe <- recipe(Y ~ ., data = df_interaction)

wf_lm_random <- workflow() %>%
    add_model(linear_spec) %>%
    add_recipe(random_recipe)

val <- get_validation_error(wf_lm_random, random_splits, "rsq") |> 
  mutate(source = "val")
tra <- get_training_error(wf_lm_random, random_splits, "rsq") |> 
    mutate(source = "tra")

data <- bind_rows(val, tra)
library(gganimate)
p <- data |> 
mutate(id = str_extract(id, "\\d+")) |>
  mutate(id = as.numeric(id)+2) |>
    ggplot(aes(x = id, y = .estimate, group = error, color = error)) +
    geom_line(color = "gray95", linewidth = 1.4) +
    geom_line() +
    scale_color_manual(values = c(Training = "#f47321", Validation = "#005030")) +
    labs(x = "",
         caption = "X axis is Training Set Size",
         y = "*R*<sup> 2</sup>",
         color = "") +
    theme_classic() +  # Apply the base theme
    plot_theme()    +
    scale_y_continuous(lim = c(0, 1))  +
    scale_x_continuous(breaks = seq(0, 400, 10)) 

anim <- p + 
  transition_reveal(id)
  
anim

anim_save("animation.gif", anim)
```



