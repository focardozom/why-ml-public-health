---
title: "Why Machine Learning is useful for Public Health"
format: revealjs
editor: visual
execute:
  eval: false
editor_options: 
  chunk_output_type: console
bibliography: ["references.bib"]
filters:
  - shinylive
---


## Agenda
:::: {.columns}

::: {.column width="40%"}

### Part I - Machine Learning

- Key Concepts
- Simulation Examples

:::

::: {.column width="60%"}

### Part II - Artificial Intelligence

- Large Lenguage Models
- Key Concepts
- Available Tools

:::

::::


## What is Public Health?

::: {.r-fit-text}
> "Public health is the art and science of preventing disease, prolonging life and promoting health through the organized efforts of society" - [@Winslow_1920]

> "Public health is primarily concerned with the health of the entire population, rather than the health of individuals. Its features include an emphasis on the promotion of health and the prevention of disease and disability; the collection and use of epidemiological data, population surveillance, and other forms of empirical quantitative assessment; a recognition of the multidimensional nature of the determinants of health; and a focus on the complex interactions of many factors - biological, behavioral, social, and environmental - in developing effective interventions." - [@Childress_2002]

*Scoping review of definitions* [@Azari_2023]

:::

## The Importance of Prediction in Public Health

::: {.r-fit-text}

What are the underlying causes of a health problem?

*What is the cause of asthma in a specific rural area?*

What are the risk factors associated with a health problem?

*Which demographic and behavioral factors increase the likelihood of developing type 2 diabetes among adults?*

::: {.fragment .highlight-red}
Can we predict the future appearance of a health problem?
:::

*Can we forecast the potential outbreak of seasonal influenza in the upcoming months?*

:::

![](memes/upstream-downstream.png)

## Machine Learning is good for Prediction

:::: {.columns}

::: {.column width="50%"}

### Traditional Statistics

- Focus on inference rather than prediction
- Limited flexibility with complex, non-linear relationships
- Emphasis on hypothesis testing and confidence intervals
- Designed for smaller datasets

:::

::: {.column width="50%"}

### Machine Learning

- Prioritizes predictive accuracy over interpretability
- Handles complex, non-linear relationships
- Uses performance metrics (e.g., accuracy, AUC, F1 score)
- Specialized techniques for unstructured data (e.g., NLP, CNNs)

:::
::::

## Machine Learning: The Best Tool for Prediction

I suggest these three concepts are at the heart of effective prediction:

- Testing error
- Regularization
- Bias-variance trade-off

Bonus: Machine learning is optimized for prediction.

# Understanding Machine Learning Advantages

## Testing error

- Measures the performance of a model on unseen data.
- Helps to evaluate the generalization ability of a model.
- The estimation of this error is key to optimizing model performance.

## Regularization

- Technique to prevent overfitting.
- Controls the complexity of the model.
- Add some bias to the model to reduce variance.

![](memes/regularization.png)

## Live Example

```{shinylive-r}
#| standalone: true
#| viewerHeight: 600
library(shiny)
library(glmnet)

# Define UI for application
ui <- fluidPage(
    titlePanel("Effect of L1 Penalty (Lasso) on Regression Slope"),
    sidebarLayout(
        sidebarPanel(
            sliderInput("penalty",
                        "Penalty (Lambda):",
                        min = 0,
                        max = 1,
                        value = 0.1,
                        step = 0.01)
        ),
        mainPanel(
           plotOutput("regressionPlot"),
           verbatimTextOutput("modelSummary")
        )
    )
)

# Define server logic
server <- function(input, output) {
    
    output$regressionPlot <- renderPlot({
        # Generate synthetic data
        set.seed(123)
        x <- matrix(rnorm(200), ncol = 2)
        y <- 3 * x[,1] + 2 * x[,2] + rnorm(100)
        
        # Fit model with Lasso penalty (alpha = 1) using both columns of x
        fit <- glmnet(x, y, alpha = 1, lambda = input$penalty)
        
        # Extract coefficients
        intercept <- coef(fit)[1]
        slope1 <- coef(fit)[2]
        slope2 <- coef(fit)[3]
        
        # Calculate predicted values using intercept and slopes
        y_pred <- intercept + x[,1] * slope1 + x[,2] * slope2
        
        # Combine data for plotting
        data <- data.frame(X1 = x[,1], Y = y, Y_Pred = y_pred)
        
        # Plot data and regression line
        par(mfrow = c(1, 2))
        
        # Plot 1: Regression line
        plot(data$X1, data$Y, main = "Lasso Regression Line (X1 vs Y)",
             xlab = "X1", ylab = "Y", pch = 19, col = "red")
        abline(intercept, slope1, col = "blue", lwd = 2)
        
        # Plot 2: Actual vs Predicted values
        plot(data$X1, data$Y, main = "Actual vs Predicted (X1 vs Y)",
             xlab = "X1", ylab = "Y", pch = 19, col = "red")
        points(data$X1, data$Y_Pred, col = "blue", pch = 19)
        
        par(mfrow = c(1, 1))
    })
    
    output$modelSummary <- renderPrint({
        # Generate synthetic data
        set.seed(123)
        x <- matrix(rnorm(200), ncol = 2)
        y <- 3 * x[,1] + 2 * x[,2] + rnorm(100)
        
        # Fit model with Lasso penalty (alpha = 1)
        fit <- glmnet(x, y, alpha = 1, lambda = input$penalty)
        
        # Display coefficients including intercept
        intercept <- coef(fit)[1]
        slope1 <- coef(fit)[2]
        slope2 <- coef(fit)[3]
        
        cat("Intercept:", intercept, "\n")
        cat("Slope for X1:", slope1, "\n")
        cat("Slope for X2:", slope2, "\n")
    })
}

# Run the application 
shinyApp(ui = ui, server = server)


```

## Bias-variance trade-off

- Bias: Error due to overly simplistic assumptions.
- Variance: Error due to overly complex models.
- The goal is to minimize the total error.

![](memes/bias-variance.png)

# Examples

## Simulation Example

Let's simulate a dataset and apply machine learning techniques to predict the outcome.

- Simulate two data sets:
  - Linear relationship
  - Non-linear relationship

- Fit a linear regression model.
- Fit a support vector machine model.
- Fit a random forest model.

## Simulation Example

Estimate these models in a few cases. Then progressively increase the sample size by one unit. Calculate the training and testing errors each time.

![](animations/methods.png)

## First Esenario

A linear problem analysed with a linear regression.
 
::: {.fragment}
![](animations/linear-linear.gif)
:::


## Second Esenario

A "non-linear" problem analysed with a linear regression.

::: {.fragment}
![](animations/linear-nonlinear.gif)
:::

## Third Esenario

A "non-linear" problem analysed with a support vector machine.

::: {.fragment}
![](animations/svm-nonlinear.gif)
:::

## Fourth Esenario

A linear problem analysed with a random forest.

::: {.fragment}
![](animations/rforest-linear.gif)
:::

# Conclusion

## Machine Learning in Public Health

- Machine learning is a powerful tool for prediction in public health.
- It prioritizes predictive accuracy over interpretability.
- It handles complex, non-linear relationships.
- It is optimized for prediction.

## Key Concepts

- Testing error: Measures the performance of a model on unseen data.
- Regularization: Technique to prevent overfitting.
- Bias-variance trade-off: The goal is to minimize the total error.


## Future Directions

- Machine learning in public health is still in its infancy.
- More research is needed to understand the potential of machine learning in public health.
- The integration of machine learning with traditional statistical approaches is promising.


## interactive Slides



# References

